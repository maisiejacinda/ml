# -*- coding: utf-8 -*-
"""berita (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L_F5WUCkopKrkUuYM673G35dM2WY96uA
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import files
uploaded = files.upload()

df = pd.read_csv("hoax_dataset_with_text.csv")

df.head()

print(df.columns)

def extract_label(judul):
    if judul.startswith("[SALAH]"):
        return "hoax"
    else:
        return "valid"

df['label'] = df['judul'].apply(extract_label)

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(data=df, x='label')
plt.title("Distribusi Label Hoaks vs Valid")
plt.show()

df.info()

print(df.columns)

"""STEP TOKENISASI IndoBERT + PROCESSING"""

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')

df['teks'] = df['teks'].astype(str)

tokenized = tokenizer(
    list(df['teks']),
    padding='max_length',
    truncation=True,
    max_length=128,
    return_tensors='pt'
)

df['teks'] = df['teks'].astype(str)

tokenized = tokenizer(
    list(df['teks']),
    padding='max_length',
    truncation=True,
    max_length=128,
    return_tensors='pt'
)

"""STEP LABEL ENCODING + TRAIN-TEST SPLIT"""

from sklearn.model_selection import train_test_split
import torch

df['label'] = df['label'].map({'hoax': 1, 'valid': 0})

input_ids = tokenized['input_ids']
attention_mask = tokenized['attention_mask']
labels = torch.tensor(df['label'].values)

X_train_ids, X_test_ids, X_train_mask, X_test_mask, y_train, y_test = train_test_split(
    input_ids,
    attention_mask,
    labels,
    test_size=0.2,
    stratify=labels,
    random_state=42
)

"""penjelasan

input_ids = hasil konversi kata ke token angka dari IndoBERT.

attention_mask = penanda token mana yang valid dan mana yang padding (0).

y_train dan y_test = label target untuk training dan evaluasi.

**STEP DEFINISI MODEL IndoBERT + CNN-LSTM**
"""

import torch
import torch.nn as nn
from transformers import BertModel

class IndoBERT_CNN_LSTM(nn.Module):
    def __init__(self):
        super().__init__()
        self.bert = BertModel.from_pretrained('indobenchmark/indobert-base-p1')
        self.conv1 = nn.Conv1d(in_channels=768, out_channels=128, kernel_size=3, padding=1)
        self.lstm = nn.LSTM(input_size=128, hidden_size=64, batch_first=True)
        self.fc = nn.Linear(64, 2)

    def forward(self, input_ids, attention_mask):
        with torch.no_grad():
            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        x = outputs.last_hidden_state
        x = x.permute(0, 2, 1)
        x = self.conv1(x)
        x = x.permute(0, 2, 1)
        _, (h_n, _) = self.lstm(x)
        logits = self.fc(h_n.squeeze(0))
        return logits

"""STEP TRAINING LOOP + EVALUASI"""

from torch.utils.data import TensorDataset, DataLoader
from sklearn.metrics import classification_report, confusion_matrix
train_dataset = TensorDataset(X_train_ids, X_train_mask, y_train)
test_dataset = TensorDataset(X_test_ids, X_test_mask, y_test)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = IndoBERT_CNN_LSTM().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)

for epoch in range(2):  # Bisa tambah epoch kalau GPU cukup kuat
    model.train()
    total_loss = 0

for batch in train_loader:
    input_ids, attention_mask, labels = [b.to(device) for b in batch]

    optimizer.zero_grad()
    outputs = model(input_ids, attention_mask)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()

    total_loss += loss.item()
    print(f"Epoch {epoch+1} - Loss: {total_loss/len(train_loader):.4f}")

"""STEP EVALUASI MODEL"""

model.eval()
y_true = []
y_pred = []
with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [b.to(device) for b in batch]

        outputs = model(input_ids, attention_mask)
        preds = torch.argmax(outputs, dim=1)

        y_true.extend(labels.cpu().numpy())
        y_pred.extend(preds.cpu().numpy())

print("Classification Report:\n")
print(classification_report(y_true, y_pred, target_names=["valid", "hoax"]))

print("Confusion Matrix:\n")
print(confusion_matrix(y_true, y_pred))

from google.colab import files
uploaded = files.upload()

df_valid = pd.read_csv("kompas_dataset_with_text.csv")

df_hoax = df

df_valid['label'] = 'valid'
df_hoax['label'] = 'hoax'

df_valid = df_valid[['teks', 'label']].dropna()
df_hoax = df_hoax[['teks', 'label']].dropna()

df_final = pd.concat([df_valid, df_hoax]).sample(frac=1, random_state=42).reset_index(drop=True)

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(data=df_final, x='label')
plt.title("Distribusi Label Hoaks vs Valid (Gabungan)")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df_final['label_enc'] = le.fit_transform(df_final['label'])
X_train, X_test, y_train, y_test = train_test_split(
    df_final['teks'], df_final['label_enc'],
    test_size=0.2, random_state=42, stratify=df_final['label_enc']
)

from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')
def tokenize_batch(texts):
    return tokenizer(
        list(texts),
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors='pt'
    )

train_encodings = tokenize_batch(X_train)
test_encodings = tokenize_batch(X_test)
import torch
y_train = torch.tensor(y_train.values)
y_test = torch.tensor(y_test.values)
X_train_ids = train_encodings['input_ids']
X_train_mask = train_encodings['attention_mask']
X_test_ids = test_encodings['input_ids']
X_test_mask = test_encodings['attention_mask']

from torch.utils.data import TensorDataset, DataLoader
train_dataset = TensorDataset(X_train_ids, X_train_mask, y_train)
test_dataset = TensorDataset(X_test_ids, X_test_mask, y_test)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16)

import torch.nn as nn

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = IndoBERT_CNN_LSTM().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)

for epoch in range(2):
    model.train()
    total_loss = 0
    for batch in train_loader:
        input_ids, attention_mask, labels = [b.to(device) for b in batch]

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1} - Loss: {total_loss / len(train_loader):.4f}")

from sklearn.metrics import classification_report, confusion_matrix

model.eval()
y_true = []
y_pred = []

with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [b.to(device) for b in batch]
        outputs = model(input_ids, attention_mask)
        preds = torch.argmax(outputs, dim=1)

        y_true.extend(labels.cpu().numpy())
        y_pred.extend(preds.cpu().numpy())

print("Classification Report:\n")
print(classification_report(y_true, y_pred, target_names=["valid", "hoax"]))

print("Confusion Matrix:\n")
print(confusion_matrix(y_true, y_pred))

"""imbang, recall tinggi di hoaks 98% artinya model deteksinya baik dan precisionnya tinggi artinya berita hoaks nya bener" hoaks. dari condusion matrix artinya ada 199 data dan yg 193 data terprediksi benar sedangkan 6 data salah

**Analisis Statistik**
"""

df_final['panjang_karakter'] = df_final['teks'].apply(len)
df_final['jumlah_kata'] = df_final['teks'].apply(lambda x: len(str(x).split()))
print(df_final[['panjang_karakter', 'jumlah_kata']].describe())

"""**Visualisasi Pendukung**"""

import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize=(10,4))
sns.histplot(data=df_final, x='panjang_karakter', hue='label', kde=True, bins=30)
plt.title("Distribusi Panjang Karakter Teks")
plt.show()
plt.figure(figsize=(8,4))
sns.boxplot(data=df_final, x='label', y='jumlah_kata')
plt.title("Sebaran Jumlah Kata per Kelas")
plt.show()

df_hoax['label'] = 'hoax'
df_valid['label'] = 'valid'

df_hoax = df_hoax[['teks', 'label']]
df_valid = df_valid[['teks', 'label']]
df = pd.concat([df_hoax, df_valid]).sample(frac=1, random_state=42).reset_index(drop=True)
df['label_enc'] = df['label'].map({'valid': 0, 'hoax': 1})

"""CLEANING TEXT

"""

import re

def clean_text(text):
    text = str(text).lower()  # lowercase
    text = re.sub(r"http\S+|www.\S+", '', text)  # hapus URL
    text = re.sub(r"\d+", '', text)  # hapus angka
    text = re.sub(r"[^\w\s]", '', text)  # hapus tanda baca
    text = re.sub(r"\s+", ' ', text).strip()  # hapus spasi ganda
    return text

# Terapkan ke kolom teks
df['teks_clean'] = df['teks'].apply(clean_text)

# Cek hasil
df[['teks', 'teks_clean']].head()

from imblearn.over_sampling import SMOTE
from sklearn.feature_extraction.text import TfidfVectorizer

# Contoh pakai TF-IDF buat vektor teks
tfidf = TfidfVectorizer(max_features=5000)
X_tfidf = tfidf.fit_transform(df['teks_clean'])
y = df['label_enc']

# Oversampling dengan SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_tfidf, y)

# Cek distribusi baru
from collections import Counter
print(Counter(y_resampled))

df.info()
df.isnull().sum()

df['label'].value_counts()
df['label'].value_counts(normalize=True) * 100

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(data=df, x='label')
plt.title("Distribusi Kelas Berita")
plt.xlabel("Label")
plt.ylabel("Jumlah")
plt.show()

df['panjang_karakter'] = df['teks'].apply(len)
df['jumlah_kata'] = df['teks'].apply(lambda x: len(str(x).split()))
df[['panjang_karakter', 'jumlah_kata']].describe()

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')

# Tokenisasi teks
tokenized = tokenizer(
    list(df['teks']),
    padding='max_length',
    truncation=True,
    max_length=512,
    return_tensors='pt'
)

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['label_enc'] = le.fit_transform(df['label'])  # hoax → 1, valid → 0

class IndoBERT_CNN_LSTM(nn.Module):
    def __init__(self):
        super().__init__()
        self.bert = BertModel.from_pretrained('indobenchmark/indobert-base-p1')
        self.conv1 = nn.Conv1d(768, 128, kernel_size=3, padding=1)
        self.lstm = nn.LSTM(128, 64, batch_first=True)
        self.fc = nn.Linear(64, 2)

    def forward(self, input_ids, attention_mask):
        with torch.no_grad():  # IndoBERT digunakan sebagai feature extractor
            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        x = outputs.last_hidden_state
        x = x.permute(0, 2, 1)  # CNN butuh format (batch, channels, seq)
        x = self.conv1(x)
        x = x.permute(0, 2, 1)  # LSTM butuh (batch, seq, channels)
        _, (h_n, _) = self.lstm(x)
        logits = self.fc(h_n.squeeze(0))
        return logits

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.histplot(df['panjang_karakter'], bins=30, kde=True, color='skyblue')
plt.title("Distribusi Panjang Karakter Berita")
plt.xlabel("Jumlah Karakter")
plt.ylabel("Jumlah Berita")
plt.grid(True)
plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import SMOTE, ADASYN
from collections import Counter

# TF-IDF vektorisasi teks
vectorizer = TfidfVectorizer(max_features=5000)
X_tfidf = vectorizer.fit_transform(df['teks_clean'])  # gunakan teks hasil cleaning
y = df['label_enc']

# Oversampling dengan SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_tfidf, y)

print("Distribusi label setelah SMOTE:", Counter(y_resampled))

# atau bisa coba ADASYN:
# adasyn = ADASYN(random_state=42)
# X_resampled, y_resampled = adasyn.fit_resample(X_tfidf, y)

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')

# Tokenisasi teks
tokenized = tokenizer(
    list(df['teks']),
    padding='max_length',
    truncation=True,
    max_length=512,
    return_tensors='pt'
)

# Hasil embedding dalam bentuk input_ids dan attention_mask
X_train_ids = train_encodings['input_ids']
X_train_mask = train_encodings['attention_mask']
X_test_ids = test_encodings['input_ids']
X_test_mask = test_encodings['attention_mask']